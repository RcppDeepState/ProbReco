---
title: "Using ProbReco with the fable package"
author: "Anastasios Panagiotelis"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ProbReco-with-fable}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The `ProbReco` package assumes that base probabilistic forecasts are available.  This vignette describes how these can be obtained using the `fable` package.  Note that the `fable` package does currently allow for probabilistic forecast reconciliation, but only under Gaussianity and not using score optimisation.  These algorithms may be integrated into future releases of `fable`.

## The data

The data `sim_hierarchy` refer to a simulated 7-variable hierarchy.  The bottom level series are all simulated from stationary ARMA models.  Noise terms are added so that the residual terms on the bottom levels have higher variance than the middle level residuals, which in turn have higher variance than the top level.  

```{r,message=F}
library(magrittr)
library(dplyr)
library(tidyr)
library(purrr)
library(fable)
library(slider)
library(ProbReco)
set.seed(1983)
sim_hierarchy
```

To ensure the results are stable we have also set a seed.

## Set up rolling window

To set up, first we should break the data into a series of rolling windows.  This can be done using the `slide` function in the `slider` package.

```{r}
#Length of window
N<-500 

#Make data windows
data_windows<-slide(sim_hierarchy,
                    ~.x,
                    .after=N-1,
                    .complete = T)
```

This creates a list, the first element of which is the data from $t=1$ to $t=500$, the second element is the data from $t=2$ to $t=501$, etc...

```{r}
data_windows[[1]]%>%head(3)
data_windows[[1]]%>%tail(3)
data_windows[[2]]%>%head(3)
data_windows[[2]]%>%tail(3)
data_windows[[500]]%>%head(3)
data_windows[[500]]%>%tail(3)
```


## Modelling and forecasting

A function for modelling and then obtaining the forecast mean and variance using data from a single window is written below.  This can be used with the map family of functions from the `purrr` package.  This function is given as

```{r}

forecast_window <- function(data_w){
  data_w%>%
    pivot_longer(cols = -Time,
               names_to = 'var',
               values_to = 'value')%>%
    as_tsibble(index = 'Time',key='var')%>%
    model(arma11=ARIMA(value~1+pdq(1,0,1)+PDQ(0,0,0)))%>%
    forecast(h=1)%>%
    arrange(match(var,c("Tot","A","B","AA","AB","BA","BB")))->f
    mean<-map_dbl(f$.distribution,use_series,mean)
    sd<-map_dbl(f$.distribution,use_series,sd)
 return(list(mean=mean,sd=sd))
}



```

Using the `fable` package requires some manipulation of the data.  So first, the data frame is converted to long format using `pivot_longer`. The data must then be converted to a `tsibble`.  Finally, the `model` function can be used from fable.  Here an ARMA(1,1) is be fit to each data set.  Note that this is only for illustrative purposes, and there will be some misspecification.  In practice the order of the ARMA can be chosen automatically.  The `forecast` function is used to obtain the forecast mean and variance for each variable.  Only 1 step ahead forecasts are obtained for this example.  The variables are arranged in the correct order and then the forecast mean and variance are extracted.  Here, dependence is completely ignored, i.e. the base forecasts are indepenedent.

To fit the model and obtain the base forecast mean and variance for each window, the `map` function can be used.  Here we will only fit models to the first 10 windows meaning that t=501 to t=510 will constitute the training data for learning the reconciliation weights.

```{r}
#Number of windows for training
W<-10
all_fc<-map(data_windows[1:W],forecast_window)
```

## Setting up arguments for reconciliation

### The S matrix

The hierarchy has the following $\boldsymbol{S}$ matrix

```{r}
S<-matrix(c(1,1,1,1,
            1,1,0,0,
            0,0,1,1,
            1,0,0,0,
            0,1,0,0,
            0,0,1,0,
            0,0,0,1),7,4,byrow = T)
```

### The realisations

The following code obtains the realisations in the form required.

```{r}

obs_i<-function(i){
  sim_hierarchy%>%
  filter(Time==i)%>%
  pivot_longer(-Time,names_to = 'var')%>%
  arrange(match(var,c("Tot","A","B","AA","AB","BA","BB")))%>%
  pull(value)
}

all_y<-map((N+1):(N+W),obs_i)

```

This list of length `r W` has the vector of true realisations from t=501 as the first element, the vector of true realisations from t=502 as the second element, etc.  Note that the `arrange` and `match` functions are used to preserve the ordering of the variables from top to bottom. Although any ordering is acceptable, the order must agree with the ordering of the rows in the $\boldsymbol{S}$ matrix.

### The list of probabilistic forecast distributions

The next step is to create a list of functions where the first element generates from the probabilistic forecast distribution at time $t=501$, the second element generates from the probabilistic forecast distribution at time $t=502$, etc.  To do this write a function that returns a function as follows

```{r}
make_genfunc<-function(input){
  f<-function(){
    fc_mean<-input$mean
    fc_sd<-input$sd
    out<-matrix(rnorm(350,mean=fc_mean,sd=fc_sd),7,50)
    return(out)
  }
  return(f)
}

```

The 'inner' function `f` generates fifty, vector-valued, one-step ahead forecasts from a independent Gaussian distributions with mean and standard deviation extracted from `input`.  The 'outer' function `make_genfunc` is required so that the entire list can be created using `map`

```{r}
all_prob<-map(all_fc,make_genfunc)
```

The object `all_prob` is in the form required.

## Using the ProbReco functions

The total score for bottom up can be found using

```{r}
  G_bu<-as.vector(rbind(matrix(0,4,4),diag(rep(1,4))))
  es_bu<-total_score(all_y,all_prob,S,G_bu)
  es_bu
```

The optimal score can be found

```{r}
  opt<-scoreopt(all_y,all_prob,S)
  opt$d
  opt$G
  opt$val
```


## Different Base Forecasts

Note that the above example assumes that the base forecasts are Gaussian and independent.  One way to relax both of these examples is to construct a probabilistic forecast based on the empirical distribution of the residuals.  To do this we must adjust the function that produces forecasts to also collect residuals.

```{r}

forecast_window_r <- function(data_w){
  data_w%>%
    pivot_longer(cols = -Time,
               names_to = 'var',
               values_to = 'value')%>%
    as_tsibble(index = 'Time',key='var')%>%
    model(arma11=ARIMA(value~1+pdq(1,0,1)+PDQ(0,0,0)))->m
    forecast(m,h=1)%>%
      arrange(match(var,c("Tot","A","B","AA","AB","BA","BB")))->f
    fc_mean<-map_dbl(f$.distribution,use_series,mean)
    residuals(m)%>%
      select(-.model)%>%
      arrange(match(var,c("Tot","A","B","AA","AB","BA","BB")))%>%
      pivot_wider(id_cols = Time,
                names_from = var,
                values_from = .resid)%>%
      select(-Time)%>%
      as.matrix()%>%t()->E
    rownames(E)<-NULL
      
 return(list(fc_mean=fc_mean,resid=E))
}
all_fc_r<-map(data_windows[1:W],forecast_window_r)

```

Note that the residuals are extracted from a data frame and converted to a matrix.  This is all done here so that it does not need to be repeated during each step of the SGD algorithm.

To construct the list of functions we adapt the `make_genfunc` to randomly choose vectors of residuals and add them to the forecast mean.

```{r}

make_genfunc_ng<-function(input){
  f<-function(){
    fc_mean<-input$fc_mean
    fc_r<-input$resid
    ind<-sample(1:ncol(fc_r),50,replace=T)
    out<-fc_r[,ind]+fc_mean
    return(out)
  }
  return(f)
}

all_prob_ng<-map(all_fc_r,make_genfunc_ng)

```

This approach will only work for one-step ahead forecasts since for multistep ahead forecasting in ARMA models a probabilistic forecast is not obtained by adding innovations to the forecast mean.  For multistep ahead forecasts the `generate` family of functions within `fable` can be used, although these are still under development for some models and can be slow.

Finally the SGD can be run to find reconciliation weights.  Note that here the flag `matches` must be set to true.  This is required since it is possible for samples drawn from the base distribution to be exactly equal.


```{r}
  opt_ng<-scoreopt(all_y,all_prob_ng,S,matches = T)
  opt_ng$d
  opt_ng$G
  
  opt_ng$val
```

The total score is lower that for the case of reconciling base forecasts. This probably indicates the importance of accounting for dependence, however an important caveat to such a conclusion is that the sample sizes are very small.